{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/park/anaconda3/envs/NLtrans/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from sklearn.metrics import accuracy_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/park/anaconda3/envs/NLtrans/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#1 tokenizer 모델 준비\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 LoRA 설정 (논문 방식대로 Q,V 가중치 적용 )\n",
    "lora_config = LoraConfig(\n",
    "    r=4,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.2,\n",
    "    target_modules=[\"query\", \"value\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\" #시퀀스 분류 작업 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 모델에 로라 적용\n",
    "lora_model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 데이터셋 로드\n",
    "dataset = load_dataset(\"glue\", \"sst2\") #감정 분류 데이터셋\n",
    "def preprocess_funtion(examples):\n",
    "    return tokenizer(examples[\"sentence\"], truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset = dataset.map(preprocess_funtion, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy 및 f1계산 함수 정의 \n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    return {\"accuracy\" : accuracy, \"f1\" : f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/park/anaconda3/envs/NLtrans/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#5 Trainer를 활용한 모델 학습\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./results_lora\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer_lora = Trainer(\n",
    "    model=lora_model,\n",
    "    args = training_args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 500/6315 [00:24<04:45, 20.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5553, 'grad_norm': 3.3077778816223145, 'learning_rate': 1.8416468725257326e-05, 'epoch': 0.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 1000/6315 [00:49<04:21, 20.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2968, 'grad_norm': 6.173166751861572, 'learning_rate': 1.6832937450514647e-05, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 1500/6315 [01:14<03:57, 20.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2828, 'grad_norm': 9.152865409851074, 'learning_rate': 1.5249406175771972e-05, 'epoch': 0.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 2000/6315 [01:40<03:35, 20.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2685, 'grad_norm': 5.13259220123291, 'learning_rate': 1.3665874901029297e-05, 'epoch': 0.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 33%|███▎      | 2109/6315 [01:46<06:25, 10.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.21919484436511993, 'eval_accuracy': 0.9151376146788991, 'eval_f1': 0.9151000768149341, 'eval_runtime': 0.6074, 'eval_samples_per_second': 1435.644, 'eval_steps_per_second': 46.099, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 2500/6315 [02:06<03:10, 20.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2627, 'grad_norm': 5.5596022605896, 'learning_rate': 1.208234362628662e-05, 'epoch': 1.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 3000/6315 [02:31<02:45, 20.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2551, 'grad_norm': 16.22600746154785, 'learning_rate': 1.0498812351543943e-05, 'epoch': 1.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 3500/6315 [02:56<02:18, 20.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2587, 'grad_norm': 3.2146382331848145, 'learning_rate': 8.915281076801267e-06, 'epoch': 1.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 4000/6315 [03:22<01:55, 19.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2519, 'grad_norm': 4.5157470703125, 'learning_rate': 7.331749802058591e-06, 'epoch': 1.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 67%|██████▋   | 4213/6315 [03:33<03:49,  9.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2154306173324585, 'eval_accuracy': 0.9220183486238532, 'eval_f1': 0.921944382510582, 'eval_runtime': 0.5976, 'eval_samples_per_second': 1459.218, 'eval_steps_per_second': 46.856, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 4500/6315 [03:47<01:29, 20.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2445, 'grad_norm': 6.649644374847412, 'learning_rate': 5.748218527315916e-06, 'epoch': 2.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 5000/6315 [04:13<01:05, 20.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2418, 'grad_norm': 6.725372791290283, 'learning_rate': 4.164687252573238e-06, 'epoch': 2.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 5500/6315 [04:38<00:40, 20.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2482, 'grad_norm': 4.537880897521973, 'learning_rate': 2.581155977830562e-06, 'epoch': 2.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 6000/6315 [05:03<00:15, 20.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2465, 'grad_norm': 5.235178470611572, 'learning_rate': 9.976247030878861e-07, 'epoch': 2.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|██████████| 6315/6315 [05:21<00:00, 19.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.20867016911506653, 'eval_accuracy': 0.9220183486238532, 'eval_f1': 0.9219752242215248, 'eval_runtime': 0.5969, 'eval_samples_per_second': 1460.842, 'eval_steps_per_second': 46.908, 'epoch': 3.0}\n",
      "{'train_runtime': 321.0203, 'train_samples_per_second': 629.39, 'train_steps_per_second': 19.672, 'train_loss': 0.28208023397486454, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6315, training_loss=0.28208023397486454, metrics={'train_runtime': 321.0203, 'train_samples_per_second': 629.39, 'train_steps_per_second': 19.672, 'total_flos': 6904336917921720.0, 'train_loss': 0.28208023397486454, 'epoch': 3.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6 로라 모델 학습.train()\n",
    "trainer_lora.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/park/anaconda3/envs/NLtrans/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#7 기존 모델 (roberta-base) 와 성능비교\n",
    "roberta_base_model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2).to(device)\n",
    "\n",
    "training_args_base = TrainingArguments(\n",
    "    output_dir=\"./results_roberta\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,    \n",
    ")\n",
    "\n",
    "trainer_base = Trainer(\n",
    "    model=roberta_base_model,\n",
    "    args=training_args,\n",
    "    train_dataset = encoded_dataset[\"train\"],\n",
    "    eval_dataset= encoded_dataset[\"validation\"],\n",
    "    tokenizer = tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 500/6315 [00:40<07:57, 12.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3321, 'grad_norm': 13.665548324584961, 'learning_rate': 1.8416468725257326e-05, 'epoch': 0.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 1000/6315 [01:23<07:14, 12.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2283, 'grad_norm': 11.887150764465332, 'learning_rate': 1.6832937450514647e-05, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 1500/6315 [02:05<06:30, 12.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2002, 'grad_norm': 15.951375007629395, 'learning_rate': 1.5249406175771972e-05, 'epoch': 0.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 2000/6315 [02:47<05:52, 12.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1878, 'grad_norm': 17.91921043395996, 'learning_rate': 1.3665874901029297e-05, 'epoch': 0.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 33%|███▎      | 2106/6315 [02:57<11:47,  5.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.19213838875293732, 'eval_accuracy': 0.9415137614678899, 'eval_f1': 0.9415056805262325, 'eval_runtime': 0.6079, 'eval_samples_per_second': 1434.543, 'eval_steps_per_second': 46.063, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 2500/6315 [03:30<05:15, 12.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1513, 'grad_norm': 3.83817982673645, 'learning_rate': 1.208234362628662e-05, 'epoch': 1.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 3000/6315 [04:12<04:34, 12.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1405, 'grad_norm': 7.973518371582031, 'learning_rate': 1.0498812351543943e-05, 'epoch': 1.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 3500/6315 [04:54<03:50, 12.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1354, 'grad_norm': 1.7485662698745728, 'learning_rate': 8.915281076801267e-06, 'epoch': 1.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 4000/6315 [05:36<03:09, 12.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1324, 'grad_norm': 1.8768327236175537, 'learning_rate': 7.331749802058591e-06, 'epoch': 1.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 67%|██████▋   | 4212/6315 [05:56<05:57,  5.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.24242940545082092, 'eval_accuracy': 0.9403669724770642, 'eval_f1': 0.9403339949929308, 'eval_runtime': 0.6089, 'eval_samples_per_second': 1432.02, 'eval_steps_per_second': 45.982, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 4500/6315 [06:19<02:28, 12.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1149, 'grad_norm': 23.693889617919922, 'learning_rate': 5.748218527315916e-06, 'epoch': 2.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 5000/6315 [07:01<01:47, 12.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1019, 'grad_norm': 10.530866622924805, 'learning_rate': 4.164687252573238e-06, 'epoch': 2.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 5500/6315 [07:44<01:06, 12.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1028, 'grad_norm': 2.3642587661743164, 'learning_rate': 2.581155977830562e-06, 'epoch': 2.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 6000/6315 [08:26<00:25, 12.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0952, 'grad_norm': 15.542317390441895, 'learning_rate': 9.976247030878861e-07, 'epoch': 2.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|██████████| 6315/6315 [08:55<00:00, 11.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.23489657044410706, 'eval_accuracy': 0.9438073394495413, 'eval_f1': 0.9437995754075569, 'eval_runtime': 0.594, 'eval_samples_per_second': 1468.007, 'eval_steps_per_second': 47.138, 'epoch': 3.0}\n",
      "{'train_runtime': 535.3253, 'train_samples_per_second': 377.428, 'train_steps_per_second': 11.797, 'train_loss': 0.15710773422712385, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6315, training_loss=0.15710773422712385, metrics={'train_runtime': 535.3253, 'train_samples_per_second': 377.428, 'train_steps_per_second': 11.797, 'total_flos': 6845227230107100.0, 'train_loss': 0.15710773422712385, 'epoch': 3.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8 기존 모델 훈련\n",
    "trainer_base.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA 모델 평가 결과:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:00<00:00, 48.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기존 RoBERTa 모델 평가 결과:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:00<00:00, 48.90it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.23489657044410706,\n",
       " 'eval_accuracy': 0.9438073394495413,\n",
       " 'eval_f1': 0.9437995754075569,\n",
       " 'eval_runtime': 0.5968,\n",
       " 'eval_samples_per_second': 1461.227,\n",
       " 'eval_steps_per_second': 46.92,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8. 성능 평가\n",
    "print(\"LoRA 모델 평가 결과:\")\n",
    "trainer_lora.evaluate()\n",
    "\n",
    "print(\"기존 RoBERTa 모델 평가 결과:\")\n",
    "trainer_base.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA 모델 파라미터 수: 739586\n",
      "기존 RoBERTa 모델 파라미터 수: 124647170\n"
     ]
    }
   ],
   "source": [
    "# 9. 모델 파라미터 수 비교\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "lora_params = count_parameters(lora_model)\n",
    "roberta_params = count_parameters(roberta_base_model)\n",
    "\n",
    "print(f\"LoRA 모델 파라미터 수: {lora_params}\")\n",
    "print(f\"기존 RoBERTa 모델 파라미터 수: {roberta_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA 모델 평가 결과:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:00<00:00, 48.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기존 RoBERTa 모델 평가 결과:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:00<00:00, 49.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA 모델 성능: Accuracy=0.9220183486238532, F1=0.9219752242215248\n",
      "기존 RoBERTa 모델 성능: Accuracy=0.9438073394495413, F1=0.9437995754075569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 10 성능 평가 및 비교 (정확도와 F1)\n",
    "print(\"LoRA 모델 평가 결과:\")\n",
    "lora_eval = trainer_lora.evaluate()\n",
    "\n",
    "print(\"기존 RoBERTa 모델 평가 결과:\")\n",
    "base_eval = trainer_base.evaluate()\n",
    "\n",
    "# 성능 출력\n",
    "print(f\"LoRA 모델 성능: Accuracy={lora_eval['eval_accuracy']}, F1={lora_eval['eval_f1']}\")\n",
    "print(f\"기존 RoBERTa 모델 성능: Accuracy={base_eval['eval_accuracy']}, F1={base_eval['eval_f1']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLtrans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
